# S8N Universal Lossless Compression - Technical Specification v2.0

1. Executive Summary

S8N is a universally lossless, dynamically adaptive compression format for f32 data, designed to achieve high compression ratios while maintaining perfect data reconstruction. It leverages the optimal sphere-packing properties of the 8-dimensional E8 lattice to quantize data into a compact set of root vectors and then losslessly encodes the residuals. S8N offers a two-level architecture: Level 1 for high-speed, partially queryable compression (4:1-8:1 expected on smooth data), and Level 2 for maximum archival compression (8:1-16:1 expected on smooth data). Its key differentiators are the mathematically principled use of vector quantization for floating-point data, guaranteed lossless reconstruction (PSNR = âˆ), and an adaptive encoding strategy that adjusts to data statistics for optimal performance.

2. Mathematical Foundation
2.1 E8 Lattice Properties

The E8 lattice is a unique, highly symmetric mathematical structure in 8-dimensional Euclidean space. It represents the densest possible packing of spheres in 8 dimensions.

Definition: The E8 lattice, denoted as
ğ¸
8
E
8
 â€‹

, is an integral lattice in
ğ‘…
8
R
8
 defined as the set of points
ğ‘¥
=

(
ğ‘¥
1
,
â€¦
,
ğ‘¥
8
)
x=(x
1
 â€‹

,â€¦,x
8
 â€‹

)
 such that:

All coordinates
ğ‘¥
ğ‘–
x
i
 â€‹

 are either integers or half-integers (i.e.,
ğ‘¥
ğ‘–
âˆˆ
ğ‘
x
i
 â€‹

âˆˆZ
 or
ğ‘¥
ğ‘–
âˆˆ
ğ‘
+
1
2
x
i
 â€‹

âˆˆZ+
2
1
 â€‹

).

The sum of the coordinates is an even integer (
âˆ‘
ğ‘–
=

1
8
ğ‘¥
ğ‘–
âˆˆ
2
ğ‘
âˆ‘
i=1
8
 â€‹

x
i
 â€‹

âˆˆ2Z
).

Basis Vectors: While multiple bases exist, a common one is derived from the D8 lattice. The E8 lattice can be generated by integer linear combinations of these basis vectors.

Root System: The most important property for S8N is its root system, which consists of the 240 shortest non-zero vectors in the lattice. These are the points closest to the origin and are called the "roots" of E8.
The 240 root vectors all have a squared norm of 2 (i.e.,
âˆ¥
ğ‘£
âˆ¥
2
=

2
âˆ¥vâˆ¥
2
=2
). They can be enumerated as follows:

112 vectors of the form
(
Â±
1
,
Â±
1
,
0
,
0
,
0
,
0
,
0
,
0
)
(Â±1,Â±1,0,0,0,0,0,0)
: All permutations of two non-zero coordinates, which are
Â±
1
Â±1
. There are
(
8
2
)
Ã—
2
2
=

28
Ã—
4
=

112
(
2
8
 â€‹

)Ã—2
2
=28Ã—4=112
 such vectors.

128 vectors of the form
(
Â±
1
2
,
Â±
1
2
,
Â±
1
2
,
Â±
1
2
,
Â±
1
2
,
Â±
1
2
,
Â±
1
2
,
Â±
1
2
)
(Â±
2
1
 â€‹

,Â±
2
1
 â€‹

,Â±
2
1
 â€‹

,Â±
2
1
 â€‹

,Â±
2
1
 â€‹

,Â±
2
1
 â€‹

,Â±
2
1
 â€‹

,Â±
2
1
 â€‹

)
: Vectors with all half-integer coordinates, where the number of negative signs is even. There are
2
8
=

256
2
8
=256
 possible sign combinations, and half of them (128) have an even number of minus signs.

The complete list of these 240 vectors is provided in Appendix A.

2.2 Scalar to 8D Coordinate Mapping

Input data Vec<f32> is processed in chunks of 8 floats. Each chunk is treated as a single vector in
ğ‘…
8
R
8
.

Algorithm:
Intent: To group a flat list of f32 scalars into 8-dimensional vectors for processing.
Inputs: A slice of f32 values, data: &[f32].
Assumptions: The input data length may not be a multiple of 8.
Steps:

Iterate through the input data slice in chunks of 8.

For each chunk, create an 8-dimensional vector
ğ‘£
=

(
ğ‘“
1
,
ğ‘“
2
,
ğ‘“
3
,
ğ‘“
4
,
ğ‘“
5
,
ğ‘“
6
,
ğ‘“
7
,
ğ‘“
8
)
v=(f
1
 â€‹

,f
2
 â€‹

,f
3
 â€‹

,f
4
 â€‹

,f
5
 â€‹

,f
6
 â€‹

,f
7
 â€‹

,f
8
 â€‹

)
.

If the last chunk has fewer than 8 elements, pad it with 0.0f32 to form a full 8D vector. The number of padding elements must be stored in the file metadata.

Pseudo-code:

code
Code
download
content_copy
expand_less
function map_to_8d_vectors(data: &[f32]) -> (Vec<[f32; 8]>, usize) {
    let num_vectors = (data.len() + 7) / 8;
    let mut vectors = Vec::with_capacity(num_vectors);
    let padding_count = (8 - (data.len() % 8)) % 8;

    for chunk in data.chunks(8) {
        let mut vector = [0.0f32; 8];
        vector[..chunk.len()].copy_from_slice(chunk);
        vectors.push(vector);
    }

    (vectors, padding_count)
}

Numerical Example:

Input: vec![1.1, -2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9]

Output:

vectors: [[1.1, -2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8], [9.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]

padding_count: 7

2.3 Residual Computation

The core of S8N is finding the nearest E8 root vector for each input vector and storing the difference (residual).

Formula: For an input vector
ğ‘£
âˆˆ
ğ‘…
8
vâˆˆR
8
 and its nearest E8 root vector
ğ‘Ÿ
âˆˆ
ğ¸
8
râˆˆE
8
 â€‹

, the residual vector
ğ‘‘
d
 is:

ğ‘‘
=

ğ‘£
âˆ’
ğ‘Ÿ
d=vâˆ’r

This computation must be performed using f32 arithmetic to ensure that the reconstruction is bit-for-bit identical.

Precision Requirements: Standard IEEE 754 single-precision floating-point arithmetic is sufficient. No special precision is required, as the reconstruction formula is simply
ğ‘£
=

ğ‘Ÿ
+
ğ‘‘
v=r+d
. As long as the same floating-point environment is used, this is perfectly reversible.

Edge Case Handling:

NaN, Infinity, -Infinity: These values are not mapped to the E8 lattice. They are handled separately. See Section 4.3 and Section 9.4. A special flag in the data stream will indicate their presence and position.

Subnormals: Handled correctly by standard IEEE 754 arithmetic.

Zero Vector: If an input vector is [0.0; 8], the nearest root is technically any of the 240 roots. The algorithm must deterministically choose one. The quantization algorithm in Section 5.3 will handle this by design.

3. Data Structures
3.1 S8NLosslessData

This is the top-level container for the compressed data.

```rust
/// Represents a compressed S8N data block.
/// This structure is designed for serialization.
#[repr(C)]
pub struct S8NLosslessData {
    /// File header containing magic number, version, etc.
    pub header: FileHeader,
    /// Metadata about the original data and compression settings.
    pub metadata: Metadata,
    /// The compressed E8 root indices.
    pub root_indices: RootIndices,
    /// The compressed residual values.
    pub residuals: Residuals,
    /// CRC32 checksum of the metadata and compressed data sections.
    pub checksum: u32,
}
```**Invariants:**
*   `metadata.original_len` must accurately reflect the number of `f32` values before compression.
*   The `checksum` must be calculated over the serialized bytes of `metadata`, `root_indices`, and `residuals`. See **Section 5.7**.

### 3.2 CompressionLevel

Defines the two primary compression strategies.

```rust
/// Specifies the compression strategy and trade-off between speed and ratio.
#[repr(u8)]
#[derive(Clone, Copy, Debug, PartialEq, Eq)]
pub enum CompressionLevel {
    /// Level 1: Fast compression, partially queryable root indices.
    /// Good for streaming and semantic search use cases.
    Level1 = 1,
    /// Level 2: Maximum compression, archival storage.
    /// Roots and residuals are interleaved and compressed together.
    Level2 = 2,
}
3.3 RootIndices

Stores the indices (0-239) of the E8 root vectors.

```rust
/// Compressed storage for the E8 root indices.
/// Each 8D vector from the input corresponds to one u8 index.
#[non_exhaustive]
pub enum RootIndices {
    /// Uncompressed indices. A fallback for incompressible data.
    Uncompressed(Vec<u8>),
    /// Indices compressed with zstd at a light level (e.g., 3).
    /// Used in Level 1 for fast decompression.
    ZstdLight(Vec<u8>),
    /// A single block containing interleaved root indices and residuals,
    /// compressed with zstd at a high level. Used only in Level 2.
    InterleavedZstdHigh(Vec<u8>),
}

Byte Layout:

Uncompressed: [data...]

ZstdLight: [compressed_data...]

InterleavedZstdHigh: This variant is a container for the final compressed blob in Level 2. The actual interleaving pattern is defined in Section 6.2.

3.4 Residuals

Stores the compressed f32 residual vectors.

```rust
/// Compressed storage for the residual vectors.
#[non_exhaustive]
pub enum Residuals {
    /// Residuals for Level 1, compressed with zstd level 19.
    /// The encoding scheme is adaptive. See Section 4.
    ZstdAdaptive(Vec<u8>),
    /// For Level 2, residuals are not stored separately.
    /// This variant indicates they are part of the InterleavedZstdHigh
    /// block in `RootIndices`.
    ContainedInRoots,
}
3.5 DataStatistics

Computed from the input data to guide adaptive encoding.

```rust
/// Statistical properties of the input data, used to guide compression.
#[repr(C)]
#[derive(Debug, Clone, Copy)]
pub struct DataStatistics {
    pub min: f32,
    pub max: f32,
    pub mean: f64, // Use f64 for precision
    pub variance: f64, // Use f64 for precision
    pub sparsity: f64, // Ratio of zero values to total values
    pub smoothness: f64, // Average absolute difference between adjacent values
    pub has_specials: bool, // True if NaN or Infinity is present
}

Calculation Algorithms: See Section 5.2.

4. Adaptive Residual Encoding

For Level 1, residuals are encoded adaptively before zstd compression to improve efficiency. For Level 2, this is not used; residuals are stored as raw f32 bytes and interleaved.

4.1 ResidualEncoding Enum

This enum defines how a residual is encoded in the byte stream passed to zstd. This is a logical enum; the output is a single byte stream.

```rust
/// Defines the encoding format for a single f32 residual value.
/// This is used to build the byte stream for zstd compression in Level 1.
#[repr(u8)]
enum ResidualEncoding {
    /// A true zero value. Encoded as a single byte `0x00`.
    Zero = 0,
    /// A small residual stored as a quantized `i16`.
    /// Layout: `0x01 [scale_idx: u8] [value: i16]`
    QuantizedI16 = 1,
    /// A medium residual stored as a quantized `i24`.
    /// This requires empirical testing to validate its utility.
    /// Layout: `0x02 [scale_idx: u8] [value: i24 as 3 bytes]`
    QuantizedI24 = 2, // Tentative, may be removed.
    /// A large residual stored as a raw f32.
    /// Layout: `0x03 [value: f32 as 4 bytes]`
    RawF32 = 3,
    /// A special value (NaN/Infinity).
    /// Layout: `0x04 [value: f32 as 4 bytes]`
    SpecialF32 = 4,
}
```

### 4.2 Threshold Selection Algorithm

**Intent:** To determine the optimal thresholds for quantizing residuals into `i16` or `i24`.
**Response:** This requires empirical testing.
**Default Algorithm (to be refined):** A simple percentile-based approach will be used as a starting point.

1. Collect all non-zero absolute residual values.
2. Sort the values.
3. The `i16` threshold (`T16`) is the value at the 80th percentile.
4. The `i24` threshold (`T24`) is the value at the 95th percentile.
5. A set of 16 scaling factors will be pre-computed. The best scale `S` is chosen to maximize the number of residuals `res` where `abs(res) / S < 32767`. The index of this scale is stored. This is a complex optimization problem. For v2.0, we will use a fixed scale derived from `T16`: `Scale = T16 / 32767.0`. Only one scale will be used per block.

### 4.3 Encoding Function

**Intent:** To encode a `Vec<[f32; 8]>` of residuals into a `Vec<u8>` for zstd.
**Inputs:** `residuals: &[[f32; 8]]`, `stats: &DataStatistics`.
**Assumptions:** Thresholds have been computed.
**Steps:**

1. Initialize an empty `Vec<u8>` for the output stream.
2. Determine the scale factor `S` from stats. Store it in the metadata.
3. For each residual `val` in the input:
    a.  If `val.is_nan()` or `val.is_infinite()`:
        *Append `ResidualEncoding::SpecialF32` byte (`0x04`).
        *   Append the 4 bytes of `val.to_le_bytes()`.
    b.  Else if `val == 0.0`:
        *Append `ResidualEncoding::Zero` byte (`0x00`).
    c.  Else if `abs(val) <= T16`:
        *   Append `ResidualEncoding::QuantizedI16` byte (`0x01`).
        *`quantized_val = (val / S).round() as i16`.
        *   Append the 2 bytes of `quantized_val.to_le_bytes()`.
    d.  Else: // For now, all others are RawF32
        *Append `ResidualEncoding::RawF32` byte (`0x03`).
        *   Append the 4 bytes of `val.to_le_bytes()`.
4. Return the output stream.

### 4.4 Decoding Function

**Intent:** To losslessly reconstruct residuals from the encoded byte stream.
**Inputs:** `encoded_data: &[u8]`, `scale_factor: f32`.
**Outputs:** A `Vec<f32>` of residuals.
**Steps:**

1. Initialize an empty `Vec<f32>` for the output.
2. Initialize a cursor `pos = 0` into `encoded_data`.
3. While `pos < encoded_data.len()`:
    a.  Read the tag byte `tag = encoded_data[pos]`.
    b.  `pos += 1`.
    c.  Match `tag`:
        *`0x00` (Zero): Push `0.0f32` to output.
        *   `0x01` (QuantizedI16):
            *Read 2 bytes for `quantized_val` (little-endian `i16`).
            *   `val = (quantized_val as f32) * scale_factor`.
            *Push `val` to output.
            *   `pos += 2`.
        *`0x03` (RawF32):
            *   Read 4 bytes for `raw_val` (little-endian `f32`).
            *Push `raw_val` to output.
            *   `pos += 4`.
        *`0x04` (SpecialF32):
            *   Read 4 bytes for `special_val` (little-endian `f32`).
            *Push `special_val` to output.
            *   `pos += 4`.
        *   `_`: Return `Error::InvalidResidualStream`.
4. Return the output vector.

## 5. Level 1 Compression Pipeline

### 5.1 Algorithm Overview

```ascii
Vec<f32> --> [ 1. Stats Analysis ] --> [ 2. 8D Mapping ] --> [ 3. E8 Quantization ]
   |                                                           |
   |                                                           v
   |                                                      [Root Indices]
   |                                                           |
   |                                                           v
   +------> [ 4. Residual Calc. ] --> [ 5. Adaptive Encode ] --+
                  |                                            |
                  v                                            v
            [Residuals]                                  [Root Indices]
                  |                                            |
                  v                                            v
[ 6. Residual zstd (L19) ]                  [ 7. Root zstd (Light) ]
                  |                                            |
                  +----------------------> [ 8. Assemble File ] --> [ 9. CRC32 ] --> S8N File
5.2 Data Statistics Analysis

Intent: To compute descriptive statistics of the input data.
Inputs: data: &[f32].
Outputs: DataStatistics struct.
Algorithm:

min, max: Single pass over the data.

mean, variance: Use Welford's online algorithm for numerical stability in a single pass.

sparsity: Single pass, count val == 0.0. sparsity = zero_count / total_count.

smoothness: Single pass, calculate sum(abs(data[i] - data[i-1])) / (len - 1).

has_specials: Single pass, check val.is_nan() || val.is_infinite().

5.3 E8 Quantization

Intent: To find the nearest E8 root vector for a given 8D input vector.
Inputs: vector: [f32; 8].
Outputs: (root_index: u8, root_vector: [f32; 8]).
Algorithm (Conway and Sloane's rounding algorithm):

Scale and Round: Let
ğ‘¦
=
ğ‘£
Ã—
2
y=vÃ—
2
 â€‹

. Round each component
ğ‘¦
ğ‘–
y
i
 â€‹

 to the nearest integer, giving a vector
ğ‘§
z
.

Calculate Residual Error: Compute the difference vector
ğ‘’
=
ğ‘¦
âˆ’
ğ‘§
e=yâˆ’z
.

Find the E8 point closest to z:
a. If the sum of components of
ğ‘§
z
 is even, the closest E8 point is
ğ‘§
/
2
z/
2
 â€‹

.
b. If the sum is odd, find the component
ğ‘’
ğ‘–
e
i
 â€‹

 with the largest absolute value. Let this be at index
ğ‘˜
k
. Let
ğ‘§
â€²
z
â€²
 be the vector
ğ‘§
z
 with its
ğ‘˜
k
-th component changed by
+
1
+1
 if
ğ‘’
ğ‘˜
<
0
e
k
 â€‹

<0
 and
âˆ’
1
âˆ’1
 if
ğ‘’
ğ‘˜
>
0
e
k
 â€‹

>0
. The closest E8 point is
ğ‘§
â€²
/
2
z
â€²
/
2
 â€‹

.

Find the root vector: The result of step 3 is the closest lattice point, not necessarily a root. To find the nearest root, we perform an exhaustive search over the 240 pre-computed root vectors. This is computationally feasible.

Iterate through all 240 roots
ğ‘Ÿ
ğ‘—
r
j
 â€‹

 from the table in Appendix A.

Compute the squared Euclidean distance:
ğ·
ğ‘—
2
=
âˆ‘
ğ‘–
=
1
8
(
ğ‘£
ğ‘–
âˆ’
ğ‘Ÿ
ğ‘—
,
ğ‘–
)
2
D
j
2
 â€‹

=âˆ‘
i=1
8
 â€‹

(v
i
 â€‹

âˆ’r
j,i
 â€‹

)
2
.

The nearest root is the one with the minimum
ğ·
ğ‘—
2
D
j
2
 â€‹

. Return its index and vector.

This exhaustive search is deterministic and guarantees finding the true nearest root vector.

5.4 Residual Computation

As defined in Section 2.3. This is a simple element-wise vector subtraction: residual[i] = input_vector[i] - root_vector[i].

5.5 Root Storage Strategy

Intent: To efficiently store the Vec<u8> of root indices for Level 1.
Response: This requires empirical testing.
Decision Criteria:

Compute the entropy of the root index stream.

If entropy > 7.5 bits/symbol, compression is unlikely to be effective. Store Uncompressed.

Otherwise, compress the stream with both lz4_flex and zstd level 3.

Choose the one that produces the smaller output. zstd is generally preferred for its higher compression ratio, even at light levels. The default will be ZstdLight.

5.6 Residual Compression

Intent: To compress the adaptively encoded residual stream.
Algorithm:

Take the Vec<u8> from the adaptive encoding step (Section 4.3).

Compress this byte vector using the zstd crate.

Parameters: Use compression level 19. Enable the checksum flag in the zstd frame.

```rust
// Example zstd invocation
fn compress_residuals_l1(data: &[u8]) -> Result<Vec<u8>, S8NError> {
    zstd::stream::encode_all(data, 19)
        .map_err(|e| S8NError::CompressionError(e.to_string()))
}
5.7 Checksum Calculation

Intent: To ensure data integrity.
Algorithm:

Serialize the Metadata struct to its byte representation.

Get the byte slices for the compressed root indices and compressed residuals.

Initialize a crc32fast::Hasher.

Update the hasher with the Metadata bytes.

Update the hasher with the compressed root index bytes.

Update the hasher with the compressed residual bytes.

Finalize the hasher to get the u32 checksum.

6. Level 2 Compression Pipeline
6.1 Algorithm Overview

```ascii
Vec<f32> --> [ 1. Stats/Map/Quantize ] --> [Root Indices] & [Raw Residuals]
                  |                                            |
                  |                                            v
                  +----------------------> [ 2. Interleave ] --> [Interleaved Stream]
                                                 |
                                                 v
                                   [ 3. Build Dictionary ] --> [zstd Dictionary]
                                                 |                     |
                                                 v                     v
                                   [ 4. Final zstd (L22) ] --> [Compressed Blob]
                                                 |
                                                 v
                                   [ 5. Assemble File ] --> [ 6. CRC32 ] --> S8N File
```

6.2 Interleaving Strategy

Intent: To arrange root and residual data to maximize zstd's effectiveness by placing related data close together.
Response: The optimal strategy requires empirical testing.
Proposed Strategy (Root-Residual Block Interleaving):

Data is processed in blocks of N vectors (e.g., N=256).

For each block, interleave the data as: [root_idx_1, res_1_f1, ..., res_1_f8, root_idx_2, res_2_f1, ..., res_2_f8, ...].

This pattern is [u8, f32, f32, f32, f32, f32, f32, f32, f32], repeated N times. Each f32 is represented by its 4 little-endian bytes.

The total size of one interleaved unit is 1 + 8 * 4 = 33 bytes.

Justification: This pattern keeps the residual vector (which is correlated with its root index) physically close in the byte stream, potentially allowing zstd's LZ77 algorithm to find matches across the root/residual boundary.

6.3 Dictionary Construction

Intent: To create a custom dictionary for zstd to improve compression of the interleaved stream.
Algorithm:

Sample the input data. For a large dataset, take the first ~128KB of the interleaved data stream.

Use the zstd::dict::from_samples function to build a dictionary.

The dictionary size should be capped (e.g., 32KB).

```rust
// Example Dictionary Construction
fn build_dictionary(interleaved_data: &[u8]) -> Vec<u8> {
    const MAX_DICT_SIZE: usize = 32 * 1024;
    const SAMPLE_SIZE: usize = 128 * 1024;
    let sample_data = &interleaved_data[..interleaved_data.len().min(SAMPLE_SIZE)];
    // zstd requires multiple samples, so we chunk the sample data
    let samples: Vec<&[u8]> = sample_data.chunks(2048).collect();
    zstd::dict::from_samples(&samples, MAX_DICT_SIZE).unwrap_or_default()
}
```

### 6.4 Final Compression

**Intent:** To perform the final compression step for Level 2.
**Algorithm:**

1. Create a `zstd::bulk::Compressor` instance, providing the dictionary built in **Section 6.3**.
2. Set the compression level to 22.
3. Compress the entire interleaved byte stream.

## 7. Universal Decompression

### 7.1 Algorithm Overview

```ascii
S8N File --> [ 1. Read Header & Meta ] --> [ 2. CRC32 Verify ] --(fail)--> ERROR
   |                                            | (pass)
   |                                            v
   +------> [ 3. Decompress Roots ] ---.
   |                                   |
   v                                   |
[ 4. Decompress Residuals ] ---------> [ 5. Reconstruct 8D Vectors ] --> [ 6. Unmap to Vec<f32> ] --> Original Data
```

7.2 Checksum Verification

Intent: To validate file integrity before decompression.
Algorithm:

Read the file header, metadata, compressed data sections, and the stored u32 checksum from the footer.

Perform the exact same CRC32 calculation as described in Section 5.7 on the read metadata and compressed data bytes.

Compare the calculated checksum with the stored checksum.

If they do not match, return S8NError::ChecksumMismatch. Do not proceed with decompression.

7.3 Root Decompression

Algorithm:

Check metadata.compression_level.

If Level1:

Match on the RootIndices enum variant.

Uncompressed(data): The data is the Vec<u8> of indices.

ZstdLight(data): Decompress data using zstd::stream::decode_all. The result is the Vec<u8> of indices.

If Level2:

The RootIndices variant must be InterleavedZstdHigh(data).

Decompress this blob using the stored dictionary (which must be part of the file format).

This decompressed stream must then be de-interleaved to separate root indices from residual bytes.

7.4 Residual Decompression

Algorithm:

If Level1:

The Residuals variant is ZstdAdaptive(data).

Decompress data using zstd::stream::decode_all.

Pass the resulting byte stream to the adaptive decoding function from Section 4.4.

If Level2:

The Residuals variant is ContainedInRoots.

The raw residual f32 bytes are extracted during the de-interleaving of the Level 2 blob. Convert these bytes back to f32 values.

7.5 Reconstruction

Intent: To perfectly reconstruct the original f32 values.
Inputs: root_indices: &[u8], residuals: &[[f32; 8]].
Outputs: Vec<[f32; 8]>.
Algorithm:

Initialize an output Vec<[f32; 8]>.

For each (i, root_index) in root_indices.iter().enumerate():
a. Look up the corresponding root vector r from the E8 table in Appendix A using root_index.
b. Get the corresponding residual vector d = residuals[i].
c. Compute the reconstructed vector v where v[j] = r[j] + d[j] for j in 0..8.
d. Push v to the output vector.

The final Vec<f32> is created by flattening this vector of vectors and truncating any padding added during compression (the padding amount is stored in the metadata).

8. Implementation Requirements
8.1 Dependencies

```toml
[dependencies]
# For zstd compression
zstd = "0.12"
# For CRC32 checksum
crc32fast = "1.3"
# For error-handling
yoshi = { path = "../yoshi" }
yoshi-derive = { path = "../yoshi-derive" }
# For serialization/deserialization (optional but recommended)
serde = { version = "1.0", features = ["derive"] }
bincode = "1.3"
# For LZ4 fallback
lz4_flex = "0.11"
```

Exact versions are critical for deterministic compression.

8.2 Safety Invariants

The core compression and decompression logic can be implemented entirely in safe Rust. unsafe should only be considered for performance-critical sections after profiling and with extreme care.

Potential unsafe block:

Vectorized (SIMD) Distance Calculation: The nearest neighbor search in Section 5.3 can be significantly accelerated using SIMD instructions (std::arch).

Safety Justification:

```rust
// Hypothetical SIMD implementation
#[cfg(target_arch = "x86_64")]
use std::arch::x86_64::*;

pub unsafe fn squared_distance_simd(a: [f32; 8], b: [f32; 8]) -> f32 {
    // Safety:
    // 1. We are on x86_64, so AVX instructions are potentially available.
    //    A runtime check for the feature is required before calling this.
    // 2. The inputs `a` and `b` are `[f32; 8]`, which are 32 bytes long,
    //    matching the size of a `__m256` register (8 * 4 bytes).
    //    The pointers derived from them will be aligned and valid for reads
    //    of 32 bytes.
    let a_vec = _mm256_loadu_ps(a.as_ptr());
    let b_vec = _mm256_loadu_ps(b.as_ptr());
    let diff = _mm256_sub_ps(a_vec, b_vec);
    let squared = _mm256_mul_ps(diff, diff);

    // Horizontal sum
    let hsum1 = _mm256_hadd_ps(squared, squared);
    let hsum2 = _mm256_hadd_ps(hsum1, hsum1);
    let perm = _mm256_permute2f128_ps(hsum2, hsum2, 1);
    let final_sum_vec = _mm_add_ss(_mm256_castps256_ps128(hsum2), _mm256_castps256_ps128(perm));
    _mm_cvtss_f32(final_sum_vec)
}

Any such block must be gated by #[cfg(target_feature = "...")] and ideally have a safe fallback.

8.3 Error Handling

A comprehensive error enum is required.

```rust
#[derive(Debug)]
#[non_exhaustive]
pub enum S8NError {
    InvalidHeader,
    UnsupportedVersion,
    ChecksumMismatch,
    CompressionError(String),
    DecompressionError(String),
    InvalidDataStream,
    IoError(std::io::Error),
    // ... other potential errors
}

impl From<std::io::Error> for S8NError {
    fn from(err: std::io::Error) -> Self {
        S8NError::IoError(err)
    }
}
```

8.4 Memory Management

Allocation: Buffers should be reused where possible. The compressor/decompressor should accept a pre-allocated output buffer to reduce allocations.

Zero-Copy: For Level 1 Uncompressed roots, it's possible to return a slice &[u8] that points directly into the input buffer, avoiding a copy. This requires careful lifetime management.

Streaming APIs: See Section 13.2. The initial implementation will focus on in-memory compression of Vec<f32>.

9. Testing Strategy
9.1 Losslessness Verification

Test Suite: roundtrip_lossless
Cases:

Zeros: vec![0.0; 1024]

Ones: vec![1.0; 1024]

Range: (0..1024).map(|i| i as f32).collect()

Sine Wave: (0..1024).map(|i| (i as f32 * 0.1).sin()).collect()

Random: Uniformly random data.

Padded: A vector whose length is not a multiple of 8.

Assertion: For each case, compress and then decompress the data. The reconstructed vector must be bit-for-bit identical to the original.
assert_eq!(original.iter().map(|f| f.to_bits()).collect::<Vec<_>>(), reconstructed.iter().map(|f| f.to_bits()).collect::<Vec<_>>());

9.2 Compression Ratio Benchmarks

Datasets:

Silesia Corpus: (e.g., silesia.tar) - Though not floats, can be converted for testing general-purpose performance.

Weather Data: Smooth, correlated floating-point data. (e.g., from NOAA).

LLM Weights: (e.g., from a small model like TinyLlama) - Represents data S8N might be good at.

Synthetic Smooth: Generated sine waves, polynomials.

Synthetic Noise: rand::thread_rng().gen::<f32>().

Acceptance Criteria:

On Weather/LLM data, Level 1 must achieve > 4:1.

On Weather/LLM data, Level 2 must achieve > 8:1.

On Synthetic Noise, the compressed size must not exceed 105% of the original size.

9.3 Performance Benchmarks

Hardware Baseline: AMD Ryzen 9 5950X, 64GB DDR4 RAM, running on a single core.
Criteria:

Level 1 Compression (Weather Data): > 200 MB/s

Level 1 Decompression (Weather Data): > 500 MB/s

Level 2 Compression (Weather Data): > 50 MB/s

Level 2 Decompression (Weather Data): > 400 MB/s

9.4 Edge Case Testing

Test Suite: edge_cases
Cases:

Empty Input: vec![] -> Should produce a valid, small S8N file.

Single Value: vec![1.23]

Special Values: vec![f32::NAN, f32::INFINITY, f32::NEG_INFINITY, -0.0, f32::MIN, f32::MAX] -> Must roundtrip perfectly.

Subnormals: Test with values close to f32::MIN_POSITIVE.

Corrupted Data: Test decompression of a file with an invalid checksum (must fail with ChecksumMismatch) and a malformed data stream (must fail with InvalidDataStream).

10. Performance Characteristics
10.1 Expected Compression Ratios
Data Type / Characteristics Level 1 Expected Ratio Level 2 Expected Ratio
Optimistic (Smooth, e.g., Sine Wave) 6:1 - 10:1 12:1 - 20:1
Realistic (LLM Weights, Weather) 4:1 - 8:1 8:1 - 16:1
Pessimistic (Uncorrelated Noise) 0.95:1 - 1.1:1 0.95:1 - 1.1:1
Pathological (Adversarial Data) ~0.9:1 ~0.9:1

Pathological Case: Data specifically crafted such that every 8D vector lies perfectly centered between two E8 roots would maximize residual magnitude, hurting compression. Uncorrelated, uniformly distributed random data is a good proxy for the pessimistic/pathological case where S8N provides no benefit.

10.2 Throughput Targets

(Single-threaded, on baseline hardware specified in Section 9.3)

Operation Target Throughput
Level 1 Compression 200 MB/s
Level 1 Decompression 500 MB/s
Level 2 Compression 50 MB/s
Level 2 Decompression 400 MB/s
10.3 Memory Usage

Let
ğ‘
N
 be the number of f32 values.

Compression: Peak memory will be roughly (N _4) (input) + (N/8_ 33) (interleaved L2) + (N _4_ 1.1) (zstd output buffer). Approximately ~1.5x the input size for Level 2, less for Level 1.

Decompression: Peak memory will be (Compressed Size) + (N * 4) (output). Approximately 1.x times the original data size, where x depends on the compression ratio.

11. Binary Format Specification
11.1 File Header
Offset Size (bytes) Field Description
0 4 Magic Number 0x45385821 (S8N!)
4 1 Version 0x02 (for this spec v2.0)
5 1 CompressionLevel 0x01 for Level 1, 0x02 for Level 2
6 2 Reserved Must be zero.
11.2 Metadata Section

This section immediately follows the header. It is a variable-length structure, but fields are fixed.

Offset Size (bytes) Field Description
8 8 Original Length Number of f32 elements (u64, little-endian)
16 1 Padding Count Number of 0.0 pads added (0-7, u8)
17 4 Root Data Size Compressed size of root indices (u32, LE)
21 4 Residual Data Size Compressed size of residuals (u32, LE)
25 1 Root Compression 0: Uncompressed, 1: ZstdLight, 2: Interleaved
26 4 Scale Factor f32 scale for adaptive residuals (Level 1 only)
30 4 Dict Size u32 size of dictionary (Level 2 only)
34 Dict Size Dictionary zstd dictionary blob (Level 2 only)
... ... ...
11.3 Compressed Data Section

The data sections follow the metadata section.

Dictionary: (Level 2 only) Starts at offset 34. Size is Dict Size.

Root Data: Starts after metadata (or dictionary). Size is Root Data Size.

Residual Data: Starts immediately after Root Data. Size is Residual Data Size. (For Level 2, this size is 0).

11.4 Checksum Footer
Offset Size (bytes) Field Description
End of file - 4 bytes 4 Checksum CRC32 of Metadata + Compressed Data
12. Reference Implementation Roadmap
12.1 Implementation Order

Core Math: Implement E8 lattice tables, nearest neighbor search. Unit test extensively.

Data Structures: Define all structs and enums.

Statistics: Implement the DataStatistics computation.

Level 1 Pipeline: Implement the full compression/decompression flow for Level 1.

Binary Format: Implement serialization and deserialization for the Level 1 format.

Testing: Build the full test suite for Level 1 (losslessness, edge cases, benchmarks).

Level 2 Pipeline: Implement interleaving, dictionary building, and Level 2 compression/decompression.

API: Expose a clean public API (compress, decompress functions).

12.2 Verification Milestones

M1: E8 quantization is bit-for-bit reproducible and verified against a known test set.

M2: Level 1 roundtrip is lossless for all test cases in Section 9.1.

M3: The serialized Level 1 binary file matches the specification. A file generated by the implementation can be parsed by a separate script.

M4: Level 2 roundtrip is lossless.

M5: All performance and compression ratio targets are met or exceeded on benchmark datasets.

13. Future Extensions
13.1 Multi-Type Support

The framework can be extended to f64 by using a scaled E8 lattice or other high-dimensional lattices like the Leech lattice (
Î›
24
Î›
24
 â€‹

). For i32, the concept is less applicable, and standard integer compression methods are likely better.

13.2 Streaming API

A streaming API would operate on blocks. The compressor would need to maintain state (e.g., zstd context). The file format would need to be chunked, with each chunk having its own header and checksum, allowing for independent decompression.

13.3 Parallel Compression

The input data can be split into large chunks, and each chunk can be compressed in parallel on a separate thread. The key challenge is ensuring deterministic output, especially for Level 2 dictionary building. A fixed sampling strategy for the dictionary or a pre-trained dictionary would be required. The final output would be a concatenation of compressed chunks.

Appendix A: E8 Lattice Root Tables

This appendix would contain the full list of the 240 root vectors, represented as [[f32; 8]; 240] in a separate file or pre-computed constant in the source code.

A sample of the roots:

```rust
const E8_ROOTS: [[f32; 8]; 240] = [
    // Type 1: (Â±1, Â±1, 0, ...)
    [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [-1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [-1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    // ... all 28*4=112 permutations
    // Type 2: (Â±0.5, Â±0.5, ...)
    [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], // 0 negative signs (even)
    [-0.5, -0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], // 2 negative signs (even)
    // ... all 128 combinations with an even number of negative signs
];
```

## Appendix B: Compression Ratio Test Results

This requires empirical testing. The table will be populated after the reference implementation is complete and benchmarked.

| Dataset          | Original Size (MB) | Level 1 Size (MB) | Level 1 Ratio | Level 2 Size (MB) | Level 2 Ratio |
| ---------------- | ------------------ | ----------------- | ------------- | ----------------- | ------------- |
| `noaa_weather.bin` | 100.0              | _TBD_             | _TBD_         | _TBD_             | _TBD_         |
| `tinylstm.weights`| 24.5               | _TBD_             | _TBD_         | _TBD_             | _TBD_         |
| `silesia.enwik8`  | 100.0              | _TBD_             | _TBD_         | _TBD_             | _TBD_         |
| `random_f32.bin`  | 64.0               | _TBD_             | _TBD_         | _TBD_             | _TBD_         |

## Appendix C: API Reference

```rust
/// Top-level S8N error enum.
#[derive(Debug)]
pub enum S8NError { /* ... as defined in 8.3 ... */ }

/// Compression level and strategy.
#[derive(Clone, Copy, Debug)]
pub enum CompressionLevel { Level1, Level2 }

/// Compresses a slice of f32 data into a byte vector.
///
/// # Arguments
/// * `data` - The input floating-point data.
/// * `level` - The desired compression level.
///
/// # Returns
/// A `Result` containing the compressed `Vec<u8>` or an `S8NError`.
///
/// # Example
/// ```
/// use s8n::{compress, CompressionLevel};
/// let data: Vec<f32> = (0..1000).map(|i| (i as f32 * 0.1).sin()).collect();
/// let compressed = compress(&data, CompressionLevel::Level1).unwrap();
/// assert!(compressed.len() < data.len() * 4);
/// ```
pub fn compress(data: &[f32], level: CompressionLevel) -> Result<Vec<u8>, S8NError> {
    // ... implementation ...
}

/// Decompresses an S8N byte slice back into a Vec<f32>.
///
/// # Arguments
/// * `compressed_data` - A slice containing data compressed with S8N.
///
/// # Returns
/// A `Result` containing the reconstructed `Vec<f32>` or an `S8NError`.
///
/// # Example
/// ```
/// use e8x::{compress, decompress, CompressionLevel};
/// let original: Vec<f32> = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0];
/// let compressed = compress(&original, CompressionLevel::Level1).unwrap();
/// let decompressed = decompress(&compressed).unwrap();
/// assert_eq!(original, decompressed);
/// ```
pub fn decompress(compressed_data: &[u8]) -> Result<Vec<f32>, S8NError> {
    // ... implementation ...
}
